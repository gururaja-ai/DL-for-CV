{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d2b303",
   "metadata": {
    "papermill": {
     "duration": 0.009683,
     "end_time": "2024-05-18T20:15:22.449317",
     "exception": false,
     "start_time": "2024-05-18T20:15:22.439634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part-1: Exploratory Data Analysis of Satellite Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21755d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:37.796454Z",
     "iopub.status.busy": "2024-05-19T09:58:37.796102Z",
     "iopub.status.idle": "2024-05-19T09:58:44.362685Z",
     "shell.execute_reply": "2024-05-19T09:58:44.361881Z",
     "shell.execute_reply.started": "2024-05-19T09:58:37.796425Z"
    },
    "papermill": {
     "duration": 8.363114,
     "end_time": "2024-05-18T20:15:30.821405",
     "exception": false,
     "start_time": "2024-05-18T20:15:22.458291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image, center_crop\n",
    "from torchvision.utils import draw_segmentation_masks, make_grid\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from webcolors import rgb_to_name\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f566a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:44.364618Z",
     "iopub.status.busy": "2024-05-19T09:58:44.364219Z",
     "iopub.status.idle": "2024-05-19T09:58:57.320460Z",
     "shell.execute_reply": "2024-05-19T09:58:57.319252Z",
     "shell.execute_reply.started": "2024-05-19T09:58:44.364594Z"
    },
    "papermill": {
     "duration": 14.452838,
     "end_time": "2024-05-18T20:15:45.283453",
     "exception": false,
     "start_time": "2024-05-18T20:15:30.830615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install webcolors -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac570e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.322569Z",
     "iopub.status.busy": "2024-05-19T09:58:57.322188Z",
     "iopub.status.idle": "2024-05-19T09:58:57.328084Z",
     "shell.execute_reply": "2024-05-19T09:58:57.327080Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.322537Z"
    },
    "papermill": {
     "duration": 0.018191,
     "end_time": "2024-05-18T20:15:45.310746",
     "exception": false,
     "start_time": "2024-05-18T20:15:45.292555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = \"/kaggle/input/deepglobe-land-cover-classification-dataset\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "valid_dir = os.path.join(base_dir, \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e480b",
   "metadata": {
    "id": "aVm7b4ehXrEG",
    "papermill": {
     "duration": 0.008661,
     "end_time": "2024-05-18T20:15:45.328465",
     "exception": false,
     "start_time": "2024-05-18T20:15:45.319804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## There data cosists of 3 dirs(train, test, valid) along with a .csv with the classes and another with the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2fac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.330624Z",
     "iopub.status.busy": "2024-05-19T09:58:57.330308Z",
     "iopub.status.idle": "2024-05-19T09:58:57.368305Z",
     "shell.execute_reply": "2024-05-19T09:58:57.367529Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.330602Z"
    },
    "papermill": {
     "duration": 0.053765,
     "end_time": "2024-05-18T20:15:45.391058",
     "exception": false,
     "start_time": "2024-05-18T20:15:45.337293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = pd.read_csv(os.path.join(base_dir, \"class_dict.csv\"))\n",
    "annotations = pd.read_csv(os.path.join(base_dir, \"metadata.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83452717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.370281Z",
     "iopub.status.busy": "2024-05-19T09:58:57.369522Z",
     "iopub.status.idle": "2024-05-19T09:58:57.383893Z",
     "shell.execute_reply": "2024-05-19T09:58:57.382883Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.370245Z"
    },
    "papermill": {
     "duration": 0.026722,
     "end_time": "2024-05-18T20:15:45.426999",
     "exception": false,
     "start_time": "2024-05-18T20:15:45.400277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_ids = annotations[annotations[\"split\"] == \"train\"][\"image_id\"].values\n",
    "transformed_masks_path = \"/kaggle/input/processed-masks/train_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977db243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.385451Z",
     "iopub.status.busy": "2024-05-19T09:58:57.385114Z",
     "iopub.status.idle": "2024-05-19T09:58:57.394051Z",
     "shell.execute_reply": "2024-05-19T09:58:57.393228Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.385426Z"
    },
    "id": "g_sQVkAHp6WD",
    "papermill": {
     "duration": 0.020583,
     "end_time": "2024-05-18T20:15:45.457565",
     "exception": false,
     "start_time": "2024-05-18T20:15:45.436982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "def show(imgs):\n",
    "    \"\"\"Helper function to show images in the torch format\"\"\"\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(100, 100))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba668b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.395632Z",
     "iopub.status.busy": "2024-05-19T09:58:57.395323Z",
     "iopub.status.idle": "2024-05-19T09:58:57.406644Z",
     "shell.execute_reply": "2024-05-19T09:58:57.405741Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.395608Z"
    },
    "papermill": {
     "duration": 0.022251,
     "end_time": "2024-05-18T20:15:45.489285",
     "exception": false,
     "start_time": "2024-05-18T20:15:45.467034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mapping from class indices to RGB\n",
    "class_to_rgb = {}\n",
    "for idx, row in classes.iterrows():\n",
    "    class_to_rgb[row[0]] = row[1:].to_list()\n",
    "class_colors = [tuple(x) for x in class_to_rgb.values()]\n",
    "num_classes = len(class_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b987ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.407944Z",
     "iopub.status.busy": "2024-05-19T09:58:57.407672Z",
     "iopub.status.idle": "2024-05-19T09:58:57.917911Z",
     "shell.execute_reply": "2024-05-19T09:58:57.916879Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.407922Z"
    },
    "id": "4tfENqjsDPh5",
    "outputId": "aa446f83-4289-40bc-8287-fa89def17fe5",
    "papermill": {
     "duration": 0.542495,
     "end_time": "2024-05-18T20:15:46.041633",
     "exception": false,
     "start_time": "2024-05-18T20:15:45.499138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of files\n",
    "print(f\"Number of files in the train dir: {len(os.listdir(train_dir))}\")\n",
    "print(f\"Number of files in the test dir : {len(os.listdir(test_dir))}\")\n",
    "print(f\"Number of files in the valid dir: {len(os.listdir(valid_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f24d1d",
   "metadata": {
    "id": "HQI_nzD7D6kf",
    "papermill": {
     "duration": 0.009495,
     "end_time": "2024-05-18T20:15:46.060963",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.051468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The satellite images are in `.jpg` format while the mask in `.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df387c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.919291Z",
     "iopub.status.busy": "2024-05-19T09:58:57.918950Z",
     "iopub.status.idle": "2024-05-19T09:58:57.925538Z",
     "shell.execute_reply": "2024-05-19T09:58:57.924514Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.919258Z"
    },
    "id": "3CRleDe7DwFT",
    "outputId": "ca8148a5-705d-4410-8bbc-de623af711a6",
    "papermill": {
     "duration": 0.019583,
     "end_time": "2024-05-18T20:15:46.090164",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.070581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# there are png and jpg files\n",
    "print(os.listdir(train_dir)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a0646",
   "metadata": {
    "id": "WOaTX7zDEoXX",
    "papermill": {
     "duration": 0.009727,
     "end_time": "2024-05-18T20:15:46.109318",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.099591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The test and valid directories don't contain masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3531d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.930506Z",
     "iopub.status.busy": "2024-05-19T09:58:57.930156Z",
     "iopub.status.idle": "2024-05-19T09:58:57.942488Z",
     "shell.execute_reply": "2024-05-19T09:58:57.941417Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.930474Z"
    },
    "id": "9iYxrfkCBC-v",
    "outputId": "516ec102-7c90-40e8-ce07-0c3152dfeeb2",
    "papermill": {
     "duration": 0.023963,
     "end_time": "2024-05-18T20:15:46.142523",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.118560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_ext(dir_path, ext):\n",
    "    n_files = len([name for name in os.listdir(dir_path) if name.endswith(ext)])\n",
    "    print(f\"Files with '{ext}' extension in {dir_path} dir: {n_files}\")\n",
    "\n",
    "\n",
    "list_ext(train_dir, \"jpg\")\n",
    "list_ext(train_dir, \"png\")\n",
    "list_ext(test_dir, \"jpg\")\n",
    "list_ext(test_dir, \"png\")\n",
    "list_ext(valid_dir, \"jpg\")\n",
    "list_ext(valid_dir, \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a95b4d",
   "metadata": {
    "id": "UMAdP4TrODDk",
    "papermill": {
     "duration": 0.008742,
     "end_time": "2024-05-18T20:15:46.160536",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.151794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `annotations` file contains the annotations of the images, including the sattelite and mask paths and the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9294b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.944641Z",
     "iopub.status.busy": "2024-05-19T09:58:57.944307Z",
     "iopub.status.idle": "2024-05-19T09:58:57.963122Z",
     "shell.execute_reply": "2024-05-19T09:58:57.962053Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.944613Z"
    },
    "id": "xCbBmW9LFX7O",
    "outputId": "4ebd97b0-90a4-46cd-a915-6ceb617f64d7",
    "papermill": {
     "duration": 0.028748,
     "end_time": "2024-05-18T20:15:46.198618",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.169870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b7f64",
   "metadata": {
    "id": "L1JF3vQEOPZU",
    "papermill": {
     "duration": 0.009454,
     "end_time": "2024-05-18T20:15:46.218457",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.209003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `classes` file contains the mappings between the land types and the pixel colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0aa25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.964828Z",
     "iopub.status.busy": "2024-05-19T09:58:57.964492Z",
     "iopub.status.idle": "2024-05-19T09:58:57.975897Z",
     "shell.execute_reply": "2024-05-19T09:58:57.974833Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.964798Z"
    },
    "id": "SRFF0LkhFRHR",
    "outputId": "86c91c3d-deb4-4c18-d396-c2ae4b87de39",
    "papermill": {
     "duration": 0.023837,
     "end_time": "2024-05-18T20:15:46.251837",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.228000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76a51b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.977885Z",
     "iopub.status.busy": "2024-05-19T09:58:57.977491Z",
     "iopub.status.idle": "2024-05-19T09:58:57.989310Z",
     "shell.execute_reply": "2024-05-19T09:58:57.988368Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.977843Z"
    },
    "id": "Cg_NWKSffEIS",
    "outputId": "c109b846-fd08-49ae-b470-99f79cbab8f2",
    "papermill": {
     "duration": 0.022618,
     "end_time": "2024-05-18T20:15:46.284723",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.262105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map classes to RGB values\n",
    "class_to_rgb = {}\n",
    "for idx, row in classes.iterrows():\n",
    "    class_to_rgb[row[0]] = row[1:].to_list()\n",
    "class_colors = [tuple(x) for x in class_to_rgb.values()]\n",
    "class_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed64d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:57.991122Z",
     "iopub.status.busy": "2024-05-19T09:58:57.990519Z",
     "iopub.status.idle": "2024-05-19T09:58:58.000806Z",
     "shell.execute_reply": "2024-05-19T09:58:57.999751Z",
     "shell.execute_reply.started": "2024-05-19T09:58:57.991090Z"
    },
    "id": "_NhtKi4QnCmz",
    "outputId": "5fea8ca5-f8d4-425d-aa22-d7b52918e882",
    "papermill": {
     "duration": 0.019346,
     "end_time": "2024-05-18T20:15:46.314232",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.294886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mapping from land type to colors\n",
    "{name: rgb_to_name(class_to_rgb[name]) for name in class_to_rgb.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970ba8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:58.003251Z",
     "iopub.status.busy": "2024-05-19T09:58:58.002338Z",
     "iopub.status.idle": "2024-05-19T09:58:58.160921Z",
     "shell.execute_reply": "2024-05-19T09:58:58.159952Z",
     "shell.execute_reply.started": "2024-05-19T09:58:58.003217Z"
    },
    "id": "4DbcA8pCH3Rp",
    "outputId": "62cd9c0a-830b-4813-9d92-123d075ebfe2",
    "papermill": {
     "duration": 0.185523,
     "end_time": "2024-05-18T20:15:46.510009",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.324486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dimmensions of the images\n",
    "sat_img = read_image(os.path.join(train_dir, f\"{image_ids[0]}_sat.jpg\"))\n",
    "mask = read_image(os.path.join(train_dir, f\"{image_ids[0]}_mask.png\"))\n",
    "print(f\"satellite image dimensions: {sat_img.shape}\")\n",
    "print(f\"mask image dimensions: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec469e0",
   "metadata": {
    "id": "luQNFBJgxzNW",
    "papermill": {
     "duration": 0.010389,
     "end_time": "2024-05-18T20:15:46.531450",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.521061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The pixels of the segmentation mask are represented in RGB colors, this is not ideal so we will transform it into values between 0 to the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e9e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:58.162523Z",
     "iopub.status.busy": "2024-05-19T09:58:58.162222Z",
     "iopub.status.idle": "2024-05-19T09:58:58.170541Z",
     "shell.execute_reply": "2024-05-19T09:58:58.169359Z",
     "shell.execute_reply.started": "2024-05-19T09:58:58.162499Z"
    },
    "id": "R7hbpMt5yUuR",
    "papermill": {
     "duration": 0.021545,
     "end_time": "2024-05-18T20:15:46.563695",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.542150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_mask(mask, class_colors):\n",
    "    \"\"\"\n",
    "    Transforms a mask from RGB format to a tensor of shape (H, W)\n",
    "    where a pixel represents the class label\n",
    "    \"\"\"\n",
    "    num_classes = len(class_colors)\n",
    "    h, w = mask.shape[1:]  # shape expected to be (C, H, W)\n",
    "    semantic_map = torch.zeros((h, w), dtype=torch.uint8)\n",
    "    # iterate over all the classes\n",
    "    for idx, color in enumerate(class_colors):\n",
    "        color = torch.tensor(color).view(3, 1, 1)  # rgb value\n",
    "        class_map = torch.all(torch.eq(mask, color), 0)\n",
    "        semantic_map[class_map] = idx\n",
    "    return semantic_map\n",
    "\n",
    "\n",
    "def ohe_mask(mask, num_classes):\n",
    "    \"\"\"Turns a label tensor of shape(H, W) to (num_classes, H, W)\"\"\"\n",
    "    return torch.permute(\n",
    "        F.one_hot(mask.type(torch.long), num_classes=num_classes).type(torch.bool),\n",
    "        (2, 0, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7429a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:58.172497Z",
     "iopub.status.busy": "2024-05-19T09:58:58.171937Z",
     "iopub.status.idle": "2024-05-19T09:58:58.184408Z",
     "shell.execute_reply": "2024-05-19T09:58:58.183453Z",
     "shell.execute_reply.started": "2024-05-19T09:58:58.172440Z"
    },
    "papermill": {
     "duration": 0.018316,
     "end_time": "2024-05-18T20:15:46.592500",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.574184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_masks(image_ids, class_colors, in_dir, out_dir):\n",
    "    for image_id in image_ids:\n",
    "        mask = read_image(os.path.join(in_dir, f\"{image_id}_mask.png\"))\n",
    "        to_pil_image(transform_mask(mask, class_colors)).save(\n",
    "            os.path.join(out_dir, f\"{image_id}_mask.png\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4270d1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:58.186386Z",
     "iopub.status.busy": "2024-05-19T09:58:58.185579Z",
     "iopub.status.idle": "2024-05-19T09:58:58.195827Z",
     "shell.execute_reply": "2024-05-19T09:58:58.194889Z",
     "shell.execute_reply.started": "2024-05-19T09:58:58.186356Z"
    },
    "id": "Vmj2Rg_RQmVy",
    "papermill": {
     "duration": 0.022795,
     "end_time": "2024-05-18T20:15:46.625509",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.602714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_sample(n=5, class_idx=None):\n",
    "    # lets plot a few samples\n",
    "    sample = []\n",
    "    for i in range(n):\n",
    "        while True:\n",
    "            id = np.random.choice(image_ids, 1)[0]\n",
    "            classes_img = np.unique(\n",
    "                read_image(os.path.join(transformed_masks_path, f\"{id}_mask.png\"))\n",
    "            )\n",
    "            if class_idx is None:\n",
    "                sample.append(id)\n",
    "                break\n",
    "            elif class_idx in classes_img:\n",
    "                sample.append(id)\n",
    "                break\n",
    "    imgs = []\n",
    "    for img_id in sample:\n",
    "        sat_img = read_image(os.path.join(train_dir, f\"{img_id}_sat.jpg\"))\n",
    "        masks = ohe_mask(\n",
    "            read_image(\n",
    "                os.path.join(transformed_masks_path, f\"{img_id}_mask.png\")\n",
    "            ).squeeze(),\n",
    "            len(class_colors),\n",
    "        )\n",
    "        mask_over_image = draw_segmentation_masks(\n",
    "            sat_img, masks=masks, alpha=0.5, colors=class_colors\n",
    "        )\n",
    "        imgs.extend([sat_img, mask_over_image])\n",
    "\n",
    "    display({name: rgb_to_name(class_to_rgb[name]) for name in class_to_rgb.keys()})\n",
    "    show(make_grid(imgs, nrow=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ffce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:58:58.197326Z",
     "iopub.status.busy": "2024-05-19T09:58:58.197062Z",
     "iopub.status.idle": "2024-05-19T09:58:58.350110Z",
     "shell.execute_reply": "2024-05-19T09:58:58.349096Z",
     "shell.execute_reply.started": "2024-05-19T09:58:58.197305Z"
    },
    "papermill": {
     "duration": 0.182813,
     "end_time": "2024-05-18T20:15:46.818375",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.635562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path and id\n",
    "transformed_masks_path = '/kaggle/input/processed-masks/train_masks'  # replace with your actual path\n",
    "id = '103730'  # replace with your actual id\n",
    "\n",
    "# Construct the file path\n",
    "file_path = os.path.join(transformed_masks_path, f\"{id}_mask.png\")\n",
    "print(file_path)  # print to check the constructed file path\n",
    "\n",
    "# Check if file exists before reading\n",
    "if os.path.exists(file_path):\n",
    "    classes_img = np.unique(read_image(file_path))\n",
    "    print(classes_img)  # print to check the unique classes in the image\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f26521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:30:31.178732Z",
     "iopub.status.busy": "2024-05-19T11:30:31.177836Z",
     "iopub.status.idle": "2024-05-19T11:31:09.413654Z",
     "shell.execute_reply": "2024-05-19T11:31:09.411944Z",
     "shell.execute_reply.started": "2024-05-19T11:30:31.178700Z"
    },
    "id": "diQ8bCA5XByV",
    "outputId": "2f963145-62f6-4705-c948-a741872154c5",
    "papermill": {
     "duration": 43.915522,
     "end_time": "2024-05-18T20:16:30.744431",
     "exception": false,
     "start_time": "2024-05-18T20:15:46.828909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_sample(class_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270011d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T09:59:37.142853Z",
     "iopub.status.busy": "2024-05-19T09:59:37.142400Z",
     "iopub.status.idle": "2024-05-19T10:00:07.611000Z",
     "shell.execute_reply": "2024-05-19T10:00:07.610168Z",
     "shell.execute_reply.started": "2024-05-19T09:59:37.142813Z"
    },
    "id": "_4uXZzBEAIsX",
    "papermill": {
     "duration": 31.934412,
     "end_time": "2024-05-18T20:17:03.406193",
     "exception": false,
     "start_time": "2024-05-18T20:16:31.471781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's try to count the number of pixels of each class\n",
    "counts = torch.zeros((7,))\n",
    "for image_id in image_ids:\n",
    "    mask = read_image(os.path.join(transformed_masks_path, f\"{image_id}_mask.png\"))\n",
    "    counts += torch.bincount(mask.view(-1), minlength=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f6326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:07.612320Z",
     "iopub.status.busy": "2024-05-19T10:00:07.612022Z",
     "iopub.status.idle": "2024-05-19T10:00:07.622349Z",
     "shell.execute_reply": "2024-05-19T10:00:07.621427Z",
     "shell.execute_reply.started": "2024-05-19T10:00:07.612296Z"
    },
    "id": "0HdgbOLHhtgq",
    "outputId": "5f3ee58f-d221-48e3-cb58-ea0fc52559d5",
    "papermill": {
     "duration": 0.730407,
     "end_time": "2024-05-18T20:17:04.850632",
     "exception": false,
     "start_time": "2024-05-18T20:17:04.120225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(counts / counts.sum()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b60c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:07.623782Z",
     "iopub.status.busy": "2024-05-19T10:00:07.623517Z",
     "iopub.status.idle": "2024-05-19T10:00:07.858678Z",
     "shell.execute_reply": "2024-05-19T10:00:07.857701Z",
     "shell.execute_reply.started": "2024-05-19T10:00:07.623760Z"
    },
    "id": "ueop0xlECCDs",
    "outputId": "464ec4c7-618d-4e8f-da4f-87fcb1538864",
    "papermill": {
     "duration": 1.04046,
     "end_time": "2024-05-18T20:17:06.614858",
     "exception": false,
     "start_time": "2024-05-18T20:17:05.574398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(class_to_rgb.keys(), counts / counts.sum())\n",
    "plt.title(\"Percentage of pixels per class in the dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c42a35",
   "metadata": {
    "papermill": {
     "duration": 0.760308,
     "end_time": "2024-05-18T20:17:08.133316",
     "exception": false,
     "start_time": "2024-05-18T20:17:07.373008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part-2: Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ece46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:07.860258Z",
     "iopub.status.busy": "2024-05-19T10:00:07.859914Z",
     "iopub.status.idle": "2024-05-19T10:00:07.864852Z",
     "shell.execute_reply": "2024-05-19T10:00:07.863461Z",
     "shell.execute_reply.started": "2024-05-19T10:00:07.860231Z"
    },
    "papermill": {
     "duration": 0.705883,
     "end_time": "2024-05-18T20:17:09.584576",
     "exception": false,
     "start_time": "2024-05-18T20:17:08.878693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#using [DeepLabV3+](https://arxiv.org/abs/1802.02611) for Land Cover Classfication from Satellite Imagery using [DeepGlobe Land Cover Classification Dataset](https://www.kaggle.com/balraj98/deepglobe-land-cover-classification-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2356d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:07.866284Z",
     "iopub.status.busy": "2024-05-19T10:00:07.865918Z",
     "iopub.status.idle": "2024-05-19T10:00:08.320690Z",
     "shell.execute_reply": "2024-05-19T10:00:08.319854Z",
     "shell.execute_reply.started": "2024-05-19T10:00:07.866259Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d880e25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:08.322701Z",
     "iopub.status.busy": "2024-05-19T10:00:08.321851Z",
     "iopub.status.idle": "2024-05-19T10:00:27.181665Z",
     "shell.execute_reply": "2024-05-19T10:00:27.180422Z",
     "shell.execute_reply.started": "2024-05-19T10:00:08.322672Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U segmentation-models-pytorch==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ec936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:27.183449Z",
     "iopub.status.busy": "2024-05-19T10:00:27.183125Z",
     "iopub.status.idle": "2024-05-19T10:00:29.466399Z",
     "shell.execute_reply": "2024-05-19T10:00:29.465411Z",
     "shell.execute_reply.started": "2024-05-19T10:00:27.183420Z"
    }
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils, deeplabv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643d3bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:29.473508Z",
     "iopub.status.busy": "2024-05-19T10:00:29.473223Z",
     "iopub.status.idle": "2024-05-19T10:00:52.681497Z",
     "shell.execute_reply": "2024-05-19T10:00:52.680281Z",
     "shell.execute_reply.started": "2024-05-19T10:00:29.473484Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip show segmentation_models_pytorch\n",
    "!pip show albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bca0e",
   "metadata": {},
   "source": [
    "### Read Data & Create train / valid splits 📁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f94ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:52.683329Z",
     "iopub.status.busy": "2024-05-19T10:00:52.682969Z",
     "iopub.status.idle": "2024-05-19T10:00:52.714376Z",
     "shell.execute_reply": "2024-05-19T10:00:52.713458Z",
     "shell.execute_reply.started": "2024-05-19T10:00:52.683283Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../input/deepglobe-land-cover-classification-dataset'\n",
    "\n",
    "metadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\n",
    "metadata_df = metadata_df[metadata_df['split']=='train']\n",
    "metadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\n",
    "metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "# Shuffle DataFrame\n",
    "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Perform 90/10 split for train / val\n",
    "valid_df = metadata_df.sample(frac=0.1, random_state=42)\n",
    "train_df = metadata_df.drop(valid_df.index)\n",
    "len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421390b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:52.716092Z",
     "iopub.status.busy": "2024-05-19T10:00:52.715657Z",
     "iopub.status.idle": "2024-05-19T10:00:52.725323Z",
     "shell.execute_reply": "2024-05-19T10:00:52.724323Z",
     "shell.execute_reply.started": "2024-05-19T10:00:52.716059Z"
    }
   },
   "outputs": [],
   "source": [
    "class_dict = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\n",
    "# Get class names\n",
    "class_names = class_dict['name'].tolist()\n",
    "# Get class RGB values\n",
    "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "print('All dataset classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39f99b",
   "metadata": {},
   "source": [
    "#### Shortlist specific classes to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f70a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:52.726903Z",
     "iopub.status.busy": "2024-05-19T10:00:52.726545Z",
     "iopub.status.idle": "2024-05-19T10:00:52.735321Z",
     "shell.execute_reply": "2024-05-19T10:00:52.734297Z",
     "shell.execute_reply.started": "2024-05-19T10:00:52.726869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "select_classes = ['urban_land', 'agriculture_land', 'rangeland', 'forest_land', 'water', 'barren_land', 'unknown']\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315297a",
   "metadata": {},
   "source": [
    "### Helper functions for viz. & one-hot encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b5bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:52.736927Z",
     "iopub.status.busy": "2024-05-19T10:00:52.736575Z",
     "iopub.status.idle": "2024-05-19T10:00:52.749192Z",
     "shell.execute_reply": "2024-05-19T10:00:52.748276Z",
     "shell.execute_reply.started": "2024-05-19T10:00:52.736903Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564f998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:52.750741Z",
     "iopub.status.busy": "2024-05-19T10:00:52.750433Z",
     "iopub.status.idle": "2024-05-19T10:00:52.761814Z",
     "shell.execute_reply": "2024-05-19T10:00:52.761077Z",
     "shell.execute_reply.started": "2024-05-19T10:00:52.750717Z"
    }
   },
   "outputs": [],
   "source": [
    "class LandCoverDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"DeepGlobe Land Cover Classification Challenge Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        df (str): DataFrame containing images / labels paths\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            df,\n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.image_paths = df['sat_image_path'].tolist()\n",
    "        self.mask_paths = df['mask_path'].tolist()\n",
    "        \n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982267c",
   "metadata": {},
   "source": [
    "#### Visualize Sample Image and Mask 📈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd5e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:52.763584Z",
     "iopub.status.busy": "2024-05-19T10:00:52.762883Z",
     "iopub.status.idle": "2024-05-19T10:00:57.103705Z",
     "shell.execute_reply": "2024-05-19T10:00:57.102787Z",
     "shell.execute_reply.started": "2024-05-19T10:00:52.763560Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = LandCoverDataset(train_df, class_rgb_values=select_class_rgb_values)\n",
    "random_idx = random.randint(0, len(dataset)-1)\n",
    "image, mask = dataset[2]\n",
    "\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e767429",
   "metadata": {},
   "source": [
    "### Defining Augmentations 🙃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75960232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:57.105137Z",
     "iopub.status.busy": "2024-05-19T10:00:57.104834Z",
     "iopub.status.idle": "2024-05-19T10:00:57.113520Z",
     "shell.execute_reply": "2024-05-19T10:00:57.112370Z",
     "shell.execute_reply.started": "2024-05-19T10:00:57.105112Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        album.RandomCrop(height=1024, width=1024, always_apply=True),\n",
    "        album.HorizontalFlip(p=0.5),\n",
    "        album.VerticalFlip(p=0.5),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    train_transform = [\n",
    "        album.CenterCrop(height=1024, width=1024, always_apply=True),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"\n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca75460",
   "metadata": {},
   "source": [
    "#### Visualize Augmented Images & Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd6158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:00:57.115267Z",
     "iopub.status.busy": "2024-05-19T10:00:57.114908Z",
     "iopub.status.idle": "2024-05-19T10:01:04.656799Z",
     "shell.execute_reply": "2024-05-19T10:01:04.655858Z",
     "shell.execute_reply.started": "2024-05-19T10:00:57.115237Z"
    }
   },
   "outputs": [],
   "source": [
    "augmented_dataset = LandCoverDataset(\n",
    "    train_df, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "random_idx = random.randint(0, len(augmented_dataset)-1)\n",
    "\n",
    "# Different augmentations on image/mask pairs\n",
    "for idx in range(3):\n",
    "    image, mask = augmented_dataset[idx]\n",
    "    visualize(\n",
    "        original_image = image,\n",
    "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "        one_hot_encoded_mask = reverse_one_hot(mask)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca92326",
   "metadata": {},
   "source": [
    "## Training DeepLabV3+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc23fb8",
   "metadata": {},
   "source": [
    "<h3><center>DeepLabV3+ Model Architecture</center></h3>\n",
    "<img src=\"https://miro.medium.com/max/1000/1*2mYfKnsX1IqCCSItxpXSGA.png\" width=\"750\" height=\"750\"/>\n",
    "<h4></h4>\n",
    "<h4><center><a href=\"https://arxiv.org/abs/1802.02611\">Image Source: DeepLabV3+ [Liang-Chieh Chen et al.]</a></center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7c9d2",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112aaca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:01:04.658717Z",
     "iopub.status.busy": "2024-05-19T10:01:04.658388Z",
     "iopub.status.idle": "2024-05-19T10:01:05.988039Z",
     "shell.execute_reply": "2024-05-19T10:01:05.987086Z",
     "shell.execute_reply.started": "2024-05-19T10:01:04.658690Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = select_classes\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "# Print model \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1880f9",
   "metadata": {},
   "source": [
    "#### Get Train / Val DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd7166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:01:05.989633Z",
     "iopub.status.busy": "2024-05-19T10:01:05.989270Z",
     "iopub.status.idle": "2024-05-19T10:01:05.998004Z",
     "shell.execute_reply": "2024-05-19T10:01:05.996978Z",
     "shell.execute_reply.started": "2024-05-19T10:01:05.989599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = LandCoverDataset(\n",
    "    train_df, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "valid_dataset = LandCoverDataset(\n",
    "    valid_df, \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# Get train and val data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554fa71",
   "metadata": {},
   "source": [
    "#### Set Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce91c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:01:05.999710Z",
     "iopub.status.busy": "2024-05-19T10:01:05.999353Z",
     "iopub.status.idle": "2024-05-19T10:01:06.032831Z",
     "shell.execute_reply": "2024-05-19T10:01:06.031978Z",
     "shell.execute_reply.started": "2024-05-19T10:01:05.999681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.00008),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth'):\n",
    "    model = torch.load('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth', map_location=DEVICE)\n",
    "    print('Loaded pre-trained DeepLabV3+ model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b97010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:01:06.034454Z",
     "iopub.status.busy": "2024-05-19T10:01:06.034128Z",
     "iopub.status.idle": "2024-05-19T10:01:06.217900Z",
     "shell.execute_reply": "2024-05-19T10:01:06.216938Z",
     "shell.execute_reply.started": "2024-05-19T10:01:06.034430Z"
    }
   },
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b490c4a",
   "metadata": {},
   "source": [
    "### Training DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adb76c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:01:06.219328Z",
     "iopub.status.busy": "2024-05-19T10:01:06.219061Z",
     "iopub.status.idle": "2024-05-19T11:03:37.633678Z",
     "shell.execute_reply": "2024-05-19T11:03:37.632437Z",
     "shell.execute_reply.started": "2024-05-19T10:01:06.219306Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    best_iou_score = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model, './best_model.pth')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab8b4d",
   "metadata": {},
   "source": [
    "### Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56cc76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:03:37.635470Z",
     "iopub.status.busy": "2024-05-19T11:03:37.635150Z",
     "iopub.status.idle": "2024-05-19T11:03:37.754514Z",
     "shell.execute_reply": "2024-05-19T11:03:37.753550Z",
     "shell.execute_reply.started": "2024-05-19T11:03:37.635440Z"
    }
   },
   "outputs": [],
   "source": [
    "# load best saved model checkpoint from the current run\n",
    "if os.path.exists('./best_model.pth'):\n",
    "    best_model = torch.load('./best_model.pth', map_location=DEVICE)\n",
    "    print('Loaded DeepLabV3+ model from this run.')\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "elif os.path.exists('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth'):\n",
    "    best_model = torch.load('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth', map_location=DEVICE)\n",
    "    print('Loaded DeepLabV3+ model from a previous commit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf967e76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:03:37.756004Z",
     "iopub.status.busy": "2024-05-19T11:03:37.755717Z",
     "iopub.status.idle": "2024-05-19T11:03:40.340379Z",
     "shell.execute_reply": "2024-05-19T11:03:40.339483Z",
     "shell.execute_reply.started": "2024-05-19T11:03:37.755980Z"
    }
   },
   "outputs": [],
   "source": [
    "# create test dataloader to be used with DeepLabV3+ model (with preprocessing operation: to_tensor(...))\n",
    "test_dataset = LandCoverDataset(\n",
    "    valid_df, \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# test dataset for visualization (without preprocessing augmentations & transformations)\n",
    "test_dataset_vis = LandCoverDataset(\n",
    "    train_df,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# get a random test image/mask index\n",
    "random_idx = random.randint(0, len(test_dataset_vis)-1)\n",
    "image, mask = test_dataset_vis[random_idx]\n",
    "\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede04b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:03:40.342373Z",
     "iopub.status.busy": "2024-05-19T11:03:40.341659Z",
     "iopub.status.idle": "2024-05-19T11:03:40.346934Z",
     "shell.execute_reply": "2024-05-19T11:03:40.346162Z",
     "shell.execute_reply.started": "2024-05-19T11:03:40.342338Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_preds_folder = 'sample_predictions/'\n",
    "if not os.path.exists(sample_preds_folder):\n",
    "    os.makedirs(sample_preds_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ab66f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:03:40.348642Z",
     "iopub.status.busy": "2024-05-19T11:03:40.348377Z",
     "iopub.status.idle": "2024-05-19T11:03:40.358018Z",
     "shell.execute_reply": "2024-05-19T11:03:40.357077Z",
     "shell.execute_reply.started": "2024-05-19T11:03:40.348620Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3a670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:03:40.359676Z",
     "iopub.status.busy": "2024-05-19T11:03:40.359422Z",
     "iopub.status.idle": "2024-05-19T11:09:49.421219Z",
     "shell.execute_reply": "2024-05-19T11:09:49.420301Z",
     "shell.execute_reply.started": "2024-05-19T11:03:40.359654Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(len(test_dataset)):\n",
    "\n",
    "    image, gt_mask = test_dataset[idx]\n",
    "    image_vis = test_dataset_vis[idx][0].astype('uint8')\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    # Predict test image\n",
    "    pred_mask = best_model(x_tensor)\n",
    "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "    # Convert pred_mask from `CHW` format to `HWC` format\n",
    "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "    # Get prediction channel corresponding to foreground\n",
    "    pred_urban_land_heatmap = pred_mask[:,:,select_classes.index('urban_land')]\n",
    "    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)\n",
    "    # Convert gt_mask from `CHW` format to `HWC` format\n",
    "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
    "    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)\n",
    "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
    "    \n",
    "    visualize(\n",
    "        original_image = image_vis,\n",
    "        ground_truth_mask = gt_mask,\n",
    "        predicted_mask = pred_mask,\n",
    "        pred_urban_land_heatmap = pred_urban_land_heatmap\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252758c3",
   "metadata": {},
   "source": [
    "### Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfca32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:09:49.423270Z",
     "iopub.status.busy": "2024-05-19T11:09:49.422613Z",
     "iopub.status.idle": "2024-05-19T11:12:15.351493Z",
     "shell.execute_reply": "2024-05-19T11:12:15.350579Z",
     "shell.execute_reply.started": "2024-05-19T11:09:49.423237Z"
    }
   },
   "outputs": [],
   "source": [
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_logs = test_epoch.run(test_dataloader)\n",
    "print(\"Evaluation on Test Data: \")\n",
    "print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n",
    "print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364cb84a",
   "metadata": {},
   "source": [
    "### Plot Dice Loss & IoU Metric for Train vs. Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6064180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:12:15.353628Z",
     "iopub.status.busy": "2024-05-19T11:12:15.352941Z",
     "iopub.status.idle": "2024-05-19T11:12:15.366958Z",
     "shell.execute_reply": "2024-05-19T11:12:15.366021Z",
     "shell.execute_reply.started": "2024-05-19T11:12:15.353592Z"
    }
   },
   "outputs": [],
   "source": [
    "train_logs_df = pd.DataFrame(train_logs_list)\n",
    "valid_logs_df = pd.DataFrame(valid_logs_list)\n",
    "train_logs_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a32add",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:12:15.368727Z",
     "iopub.status.busy": "2024-05-19T11:12:15.368337Z",
     "iopub.status.idle": "2024-05-19T11:12:16.044680Z",
     "shell.execute_reply": "2024-05-19T11:12:16.043772Z",
     "shell.execute_reply.started": "2024-05-19T11:12:15.368692Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('IoU Score', fontsize=20)\n",
    "plt.title('IoU Score Plot', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('iou_score_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3e06c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:12:16.046478Z",
     "iopub.status.busy": "2024-05-19T11:12:16.045966Z",
     "iopub.status.idle": "2024-05-19T11:12:16.670574Z",
     "shell.execute_reply": "2024-05-19T11:12:16.669671Z",
     "shell.execute_reply.started": "2024-05-19T11:12:16.046443Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Dice Loss', fontsize=20)\n",
    "plt.title('Dice Loss Plot', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('dice_loss_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a0943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46272a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72dad0b7",
   "metadata": {},
   "source": [
    "# method 02: Landcover Classification on DeepGlobe with NestedUnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f467df",
   "metadata": {},
   "source": [
    "## Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7565bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:12:16.672159Z",
     "iopub.status.busy": "2024-05-19T11:12:16.671812Z",
     "iopub.status.idle": "2024-05-19T11:14:40.278220Z",
     "shell.execute_reply": "2024-05-19T11:14:40.276886Z",
     "shell.execute_reply.started": "2024-05-19T11:12:16.672130Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/PyTorchLightning/pytorch-lightning\n",
    "!pip install -q git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "!pip install -q git+https://github.com/albumentations-team/albumentations\n",
    "!pip install -q torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a0370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:40.281066Z",
     "iopub.status.busy": "2024-05-19T11:14:40.280118Z",
     "iopub.status.idle": "2024-05-19T11:14:43.102188Z",
     "shell.execute_reply": "2024-05-19T11:14:43.101279Z",
     "shell.execute_reply.started": "2024-05-19T11:14:40.281009Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719c0ec",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63aad6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:43.103589Z",
     "iopub.status.busy": "2024-05-19T11:14:43.103305Z",
     "iopub.status.idle": "2024-05-19T11:14:43.116099Z",
     "shell.execute_reply": "2024-05-19T11:14:43.114922Z",
     "shell.execute_reply.started": "2024-05-19T11:14:43.103564Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 320\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "\n",
    "color_dict = pd.read_csv('../input/deepglobe-land-cover-classification-dataset/class_dict.csv')\n",
    "CLASSES = color_dict['name']\n",
    "print(color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ccdc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:43.117285Z",
     "iopub.status.busy": "2024-05-19T11:14:43.117011Z",
     "iopub.status.idle": "2024-05-19T11:14:43.138573Z",
     "shell.execute_reply": "2024-05-19T11:14:43.137733Z",
     "shell.execute_reply.started": "2024-05-19T11:14:43.117263Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "pd_dataset = pd.DataFrame({\n",
    "    'IMAGES': sorted(glob(\"../input/deepglobe-land-cover-classification-dataset/train/*.jpg\")), \n",
    "    'MASKS': sorted(glob(\"../input/deepglobe-land-cover-classification-dataset/train/*.png\"))\n",
    "})\n",
    "pd_dataset = shuffle(pd_dataset)\n",
    "pd_dataset.reset_index(inplace=True, drop=True)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb1596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:43.139882Z",
     "iopub.status.busy": "2024-05-19T11:14:43.139623Z",
     "iopub.status.idle": "2024-05-19T11:14:43.148710Z",
     "shell.execute_reply": "2024-05-19T11:14:43.147775Z",
     "shell.execute_reply.started": "2024-05-19T11:14:43.139859Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd_train, pd_test = train_test_split(pd_dataset, test_size=0.25, random_state=0)\n",
    "pd_train, pd_val = train_test_split(pd_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Training set size:\", len(pd_train))\n",
    "print(\"Validation set size:\", len(pd_val))\n",
    "print(\"Testing set size:\", len(pd_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81619b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:43.150392Z",
     "iopub.status.busy": "2024-05-19T11:14:43.150061Z",
     "iopub.status.idle": "2024-05-19T11:14:45.992803Z",
     "shell.execute_reply": "2024-05-19T11:14:45.991839Z",
     "shell.execute_reply.started": "2024-05-19T11:14:43.150363Z"
    }
   },
   "outputs": [],
   "source": [
    "index = 200\n",
    "\n",
    "sample_img = cv2.imread(pd_train.iloc[index].IMAGES)\n",
    "sample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sample_msk = cv2.imread(pd_train.iloc[index].MASKS)\n",
    "sample_msk = cv2.cvtColor(sample_msk, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
    "\n",
    "ax1.set_title('IMAGE')\n",
    "ax1.imshow(sample_img)\n",
    "\n",
    "ax2.set_title('MASK')\n",
    "ax2.imshow(sample_msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52452054",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8645e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:45.994773Z",
     "iopub.status.busy": "2024-05-19T11:14:45.994163Z",
     "iopub.status.idle": "2024-05-19T11:14:46.003496Z",
     "shell.execute_reply": "2024-05-19T11:14:46.002541Z",
     "shell.execute_reply.started": "2024-05-19T11:14:45.994738Z"
    }
   },
   "outputs": [],
   "source": [
    "def rgb2category(rgb_mask):\n",
    "    category_mask = np.zeros(rgb_mask.shape[:2], dtype=np.int8)\n",
    "    for i, row in color_dict.iterrows():\n",
    "        category_mask += (np.all(rgb_mask.reshape((-1, 3)) == (row['r'], row['g'], row['b']), axis=1).reshape(rgb_mask.shape[:2]) * i)\n",
    "    return category_mask\n",
    "\n",
    "def category2rgb(category_mask):\n",
    "    rgb_mask = np.zeros(category_mask.shape[:2] + (3,))\n",
    "    for i, row in color_dict.iterrows():\n",
    "        rgb_mask[category_mask==i] = (row['r'], row['g'], row['b'])\n",
    "    return np.uint8(rgb_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfff4f6",
   "metadata": {},
   "source": [
    "## Data Augmentations & Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06809e52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:46.004995Z",
     "iopub.status.busy": "2024-05-19T11:14:46.004689Z",
     "iopub.status.idle": "2024-05-19T11:14:46.016356Z",
     "shell.execute_reply": "2024-05-19T11:14:46.015218Z",
     "shell.execute_reply.started": "2024-05-19T11:14:46.004969Z"
    }
   },
   "outputs": [],
   "source": [
    "import albumentations as aug\n",
    "\n",
    "train_augment = aug.Compose([\n",
    "    aug.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    aug.HorizontalFlip(p=0.5),\n",
    "    aug.VerticalFlip(p=0.5),\n",
    "    aug.RandomBrightnessContrast(p=0.3)\n",
    "])\n",
    "\n",
    "test_augment = aug.Compose([\n",
    "    aug.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    aug.RandomBrightnessContrast(p=0.3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d406d",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3347192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:46.018214Z",
     "iopub.status.busy": "2024-05-19T11:14:46.017825Z",
     "iopub.status.idle": "2024-05-19T11:14:46.028483Z",
     "shell.execute_reply": "2024-05-19T11:14:46.027663Z",
     "shell.execute_reply.started": "2024-05-19T11:14:46.018182Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, augmentations=None):\n",
    "        self.df = df\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        image = cv2.imread(row.IMAGES)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(row.MASKS)\n",
    "        mask = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            data = self.augmentations(image=image, mask=mask)\n",
    "            image = data['image']\n",
    "            mask = data['mask']\n",
    "        \n",
    "        mask = rgb2category(mask)\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float64)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "        image = torch.Tensor(image) / 255.0\n",
    "        mask = torch.Tensor(mask).long()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76300c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:46.029726Z",
     "iopub.status.busy": "2024-05-19T11:14:46.029472Z",
     "iopub.status.idle": "2024-05-19T11:14:46.042418Z",
     "shell.execute_reply": "2024-05-19T11:14:46.041529Z",
     "shell.execute_reply.started": "2024-05-19T11:14:46.029694Z"
    }
   },
   "outputs": [],
   "source": [
    "class SegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, pd_train, pd_val, pd_test, batch_size=10):\n",
    "        super().__init__()\n",
    "        self.pd_train = pd_train\n",
    "        self.pd_val = pd_val\n",
    "        self.pd_test = pd_test\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SegmentationDataset(self.pd_train, train_augment)\n",
    "        self.val_dataset = SegmentationDataset(self.pd_val, test_augment)\n",
    "        self.test_dataset = SegmentationDataset(self.pd_test, test_augment)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size // 2, shuffle=False, num_workers=1)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size // 2, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742bd3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:46.043731Z",
     "iopub.status.busy": "2024-05-19T11:14:46.043423Z",
     "iopub.status.idle": "2024-05-19T11:14:46.051617Z",
     "shell.execute_reply": "2024-05-19T11:14:46.050823Z",
     "shell.execute_reply.started": "2024-05-19T11:14:46.043703Z"
    }
   },
   "outputs": [],
   "source": [
    "data_module = SegmentationDataModule(pd_train, pd_val, pd_test, batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2e163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:46.052965Z",
     "iopub.status.busy": "2024-05-19T11:14:46.052656Z",
     "iopub.status.idle": "2024-05-19T11:14:52.442099Z",
     "shell.execute_reply": "2024-05-19T11:14:52.441021Z",
     "shell.execute_reply.started": "2024-05-19T11:14:46.052942Z"
    }
   },
   "outputs": [],
   "source": [
    "image, mask = next(iter(data_module.train_dataloader()))\n",
    "image.shape, mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a071a",
   "metadata": {},
   "source": [
    "## Build Loss and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd24a991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:52.444373Z",
     "iopub.status.busy": "2024-05-19T11:14:52.443946Z",
     "iopub.status.idle": "2024-05-19T11:14:52.465886Z",
     "shell.execute_reply": "2024-05-19T11:14:52.465194Z",
     "shell.execute_reply.started": "2024-05-19T11:14:52.444324Z"
    }
   },
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import UnetPlusPlus\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "from segmentation_models_pytorch.metrics import get_stats, iou_score, accuracy, precision, recall, f1_score\n",
    "\n",
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = UnetPlusPlus(\n",
    "            encoder_name=\"timm-regnety_120\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=len(CLASSES),\n",
    "            activation=\"softmax\"\n",
    "        )\n",
    "        self.criterion = DiceLoss(mode=\"multiclass\", from_logits=False)\n",
    "    \n",
    "    def forward(self, inputs, targets=None):\n",
    "        outputs = self.model(inputs)\n",
    "        if targets is not None:\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            tp, fp, fn, tn = get_stats(outputs.argmax(dim=1).unsqueeze(1).type(torch.int64), targets, mode='multiclass', num_classes=len(CLASSES))\n",
    "            metrics = {\n",
    "                \"Accuracy\": accuracy(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
    "                \"IoU\": iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
    "                \"Precision\": precision(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
    "                \"Recall\": recall(tp, fp, fn, tn, reduction=\"micro-imagewise\"),\n",
    "                \"F1score\": f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "            }\n",
    "            return loss, metrics, outputs\n",
    "        else: \n",
    "            return outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.0001)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "\n",
    "        loss, metrics, outputs = self(images, masks)\n",
    "        self.log_dict({\n",
    "            \"train/Loss\": loss,\n",
    "            \"train/IoU\": metrics['IoU'],\n",
    "            \"train/Accuracy\": metrics['Accuracy'],\n",
    "            \"train/Precision\": metrics['Precision'],\n",
    "            \"train/Recall\": metrics['Recall'],\n",
    "            \"train/F1score\": metrics['F1score']\n",
    "        }, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "\n",
    "        loss, metrics, outputs = self(images, masks)\n",
    "        self.log_dict({\n",
    "            \"val/Loss\": loss,\n",
    "            \"val/IoU\": metrics['IoU'],\n",
    "            \"val/Accuracy\": metrics['Accuracy'],\n",
    "            \"val/Precision\": metrics['Precision'],\n",
    "            \"val/Recall\": metrics['Recall'],\n",
    "            \"val/F1score\": metrics['F1score']\n",
    "        }, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "\n",
    "        loss, metrics, outputs = self(images, masks)\n",
    "        self.log_dict({\n",
    "            \"test/Loss\": loss,\n",
    "            \"test/IoU\": metrics['IoU'],\n",
    "            \"test/Accuracy\": metrics['Accuracy'],\n",
    "            \"test/Precision\": metrics['Precision'],\n",
    "            \"test/Recall\": metrics['Recall'],\n",
    "            \"test/F1score\": metrics['F1score']\n",
    "        }, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bc4d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:52.467292Z",
     "iopub.status.busy": "2024-05-19T11:14:52.466990Z",
     "iopub.status.idle": "2024-05-19T11:14:58.927476Z",
     "shell.execute_reply": "2024-05-19T11:14:58.926575Z",
     "shell.execute_reply.started": "2024-05-19T11:14:52.467269Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = SegmentationModel()\n",
    "summary(model, input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf706a38",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9ee0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:58.929476Z",
     "iopub.status.busy": "2024-05-19T11:14:58.928864Z",
     "iopub.status.idle": "2024-05-19T11:14:59.535233Z",
     "shell.execute_reply": "2024-05-19T11:14:59.534347Z",
     "shell.execute_reply.started": "2024-05-19T11:14:58.929439Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val/F1score\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"lightning_logs\", name=\"landcover-classification-log\")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val/Accuracy\", patience=5)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    log_every_n_steps=31,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd60ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:14:59.536932Z",
     "iopub.status.busy": "2024-05-19T11:14:59.536521Z",
     "iopub.status.idle": "2024-05-19T11:21:23.456882Z",
     "shell.execute_reply": "2024-05-19T11:21:23.455779Z",
     "shell.execute_reply.started": "2024-05-19T11:14:59.536902Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344097f",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dbdbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:21:23.459502Z",
     "iopub.status.busy": "2024-05-19T11:21:23.458535Z",
     "iopub.status.idle": "2024-05-19T11:22:01.354460Z",
     "shell.execute_reply": "2024-05-19T11:22:01.353469Z",
     "shell.execute_reply.started": "2024-05-19T11:21:23.459468Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182e487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T11:22:01.356417Z",
     "iopub.status.busy": "2024-05-19T11:22:01.356053Z",
     "iopub.status.idle": "2024-05-19T11:22:03.026967Z",
     "shell.execute_reply": "2024-05-19T11:22:03.026092Z",
     "shell.execute_reply.started": "2024-05-19T11:22:01.356380Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"./lightning_logs/landcover-classification-log/version_0/metrics.csv\")\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "axes = [ax1, ax2, ax3, ax4, ax5, ax6]\n",
    "names = ['Loss', 'IoU', 'Accuracy', 'Precision', 'Recall', 'F1score']\n",
    "\n",
    "for axis, name in zip(axes, names):\n",
    "    axis.plot(metrics[f'train/{name}'].dropna())\n",
    "    axis.plot(metrics[f'val/{name}'].dropna())\n",
    "    axis.set_title(f'{name}: Train/Val')\n",
    "    axis.set_ylabel(name)\n",
    "    axis.set_xlabel('Epoch')\n",
    "    ax1.legend(['training', 'validation'], loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41307f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2b3aa76",
   "metadata": {},
   "source": [
    "# Unet++ Inferencing bench marking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7e178",
   "metadata": {},
   "source": [
    "Installing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch\n",
    "!pip install pytorch-lightning\n",
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1485e",
   "metadata": {},
   "source": [
    "Making the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import ssl\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import multiprocessing as mp\n",
    "from torch import nn\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import pandas as pd\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefe443",
   "metadata": {},
   "source": [
    "Assuring that the GPU has it memory free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5260cd",
   "metadata": {},
   "source": [
    "Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'efficientnet-b3'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CHANNELS = 3\n",
    "CLASSES = 7\n",
    "ACTIVATION = 'sigmoid'\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = (512,512)\n",
    "DEVICE, NUM_DEVICES = (\"cuda\", torch.cuda.device_count()) if torch.cuda.is_available() else (\"cpu\", mp.cpu_count())\n",
    "WORKERS = mp.cpu_count()\n",
    "print(f'Running on {NUM_DEVICES} {DEVICE}(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a665729",
   "metadata": {},
   "source": [
    "Determining the training, validation and test sets using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../input/deepglobe-land-cover-classification-dataset'\n",
    "\n",
    "metadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\n",
    "metadata_df = metadata_df[metadata_df['split']=='train']\n",
    "metadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\n",
    "metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "# Shuffle DataFrame\n",
    "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Get 90% to train, 10% to validate and 10% to test\n",
    "valid_df = metadata_df.sample(frac=0.1, random_state=42)\n",
    "minus_valid_df = metadata_df.drop(valid_df.index)\n",
    "test_df = minus_valid_df.sample(frac=0.1, random_state=42)\n",
    "train_df = minus_valid_df.drop(test_df.index)\n",
    "\n",
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74deb605",
   "metadata": {},
   "source": [
    "Determining class names and rgb related colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebd36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\n",
    "# Get class names\n",
    "class_names = class_df['name'].tolist()\n",
    "# Get class RGB values\n",
    "class_rgb_values = class_df[['r','g','b']].values.tolist()\n",
    "\n",
    "print('All dataset classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dcc98f",
   "metadata": {},
   "source": [
    "Helper functions to perform the one hot transformation on the masks. This is needed because the masks are multiclass rgb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = 0)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308bc4f",
   "metadata": {},
   "source": [
    "Implementing the Segmentation Model Class (subclassing pl.LightningModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc77aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, net, loss, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        self.loss = loss\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def shared_step(self, preds, labels):\n",
    "        # first compute statistics for true positives, false positives, false negative and\n",
    "        # true negative \"pixels\"\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(preds, labels.long(), mode='binary', threshold=0.5)\n",
    "\n",
    "        # then compute metrics with required reduction (see metric docs)\n",
    "        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "        recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "\n",
    "        return {'iou_score': iou_score,\n",
    "                'f1_score': f1_score,\n",
    "                'accuracy': accuracy,\n",
    "                'recall': recall}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "\n",
    "        metrics = self.shared_step(preds, labels)\n",
    "\n",
    "        self.log('train_iou_score', metrics['iou_score'], on_epoch=True)     \n",
    "        self.log('train_f1_score', metrics['f1_score'], on_epoch=True)     \n",
    "        self.log('train_accuracy', metrics['accuracy'], on_epoch=True)     \n",
    "        self.log('train_recall', metrics['recall'], on_epoch=True)     \n",
    "\n",
    "        loss = self.loss(preds, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "\n",
    "        metrics = self.shared_step(preds, labels)\n",
    "\n",
    "        self.log('valid_iou_score', metrics['iou_score'], on_epoch=True)     \n",
    "        self.log('valid_f1_score', metrics['f1_score'], on_epoch=True)     \n",
    "        self.log('valid_accuracy', metrics['accuracy'], on_epoch=True)     \n",
    "        self.log('valid_recall', metrics['recall'], on_epoch=True)\n",
    "\n",
    "        loss = self.loss(preds, labels)\n",
    "        self.log(\"valid_loss\", loss)             \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "\n",
    "        metrics = self.shared_step(preds, labels)\n",
    "\n",
    "        self.log('test_iou_score', metrics['iou_score'], on_epoch=True)     \n",
    "        self.log('test_f1_score', metrics['f1_score'], on_epoch=True)     \n",
    "        self.log('test_accuracy', metrics['accuracy'], on_epoch=True)     \n",
    "        self.log('test_recall', metrics['recall'], on_epoch=True)\n",
    "\n",
    "        loss = self.loss(preds, labels)\n",
    "        self.log(\"test_loss\", loss) \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        return [opt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbffa9",
   "metadata": {},
   "source": [
    "Implementing the DeepGlobe Dataset Class (subclassing Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGlobeSet(Dataset):\n",
    "    def __init__(self, df, transform=None, preprocess_fn=None, class_rgb_values=None):\n",
    "        self.image_paths = df['sat_image_path'].tolist()\n",
    "        self.mask_paths = df['mask_path'].tolist()\n",
    "        self.transform = transform\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_paths[idx]\n",
    "        mask_file = self.mask_paths[idx]\n",
    "\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "        img = np.array(img)\n",
    "\n",
    "        mask = Image.open(mask_file).convert('RGB')\n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "\n",
    "        if self.preprocess_fn:\n",
    "            img = self.preprocess_fn(img)\n",
    "            img = np.array(img, dtype=np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d06b47",
   "metadata": {},
   "source": [
    "Implementing the DeepGlobe DataModule Class (subclassing pl.LightningDataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a236bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGlobeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, valid_df, test_df, batch_size=2, img_size=(256,256), preprocess_fn=None, class_rgb_values=None):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(size=self.img_size),\n",
    "        ])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.trainset = DeepGlobeSet(self.train_df, transform=self.transform, preprocess_fn=self.preprocess_fn, class_rgb_values=self.class_rgb_values)\n",
    "        self.validset = DeepGlobeSet(self.valid_df, transform=self.transform, preprocess_fn=self.preprocess_fn, class_rgb_values=self.class_rgb_values)\n",
    "        self.testset = DeepGlobeSet(self.test_df, transform=self.transform, preprocess_fn=self.preprocess_fn, class_rgb_values=self.class_rgb_values)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testset, batch_size=self.batch_size, shuffle=False, num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9785da",
   "metadata": {},
   "source": [
    "* Defining the net that will be used in our model (Unet++)\n",
    "* Defining the loss function\n",
    "* Defining the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "net = smp.UnetPlusPlus(\n",
    "    encoder_name=ENCODER,                # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=ENCODER_WEIGHTS,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=CHANNELS,                       # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=CLASSES,                           # model output channels (number of classes in your dataset)\n",
    "    activation= ACTIVATION                  \n",
    ")\n",
    "\n",
    "preprocess_input = get_preprocessing_fn(ENCODER, pretrained=ENCODER_WEIGHTS)\n",
    "\n",
    "LOSS = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c1867",
   "metadata": {},
   "source": [
    "Defining our trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93791b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EPOCHS = 27\n",
    "OUTPUT_DIR = '/kaggle/working/'\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='valid_loss', \n",
    "    min_delta=0.00001, \n",
    "    patience=5, \n",
    "    mode='min')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    every_n_epochs=1,\n",
    "    dirpath=OUTPUT_DIR,\n",
    "    filename='lightning_trained'\n",
    ")\n",
    "\n",
    "logger = CSVLogger(OUTPUT_DIR, name='lightning_logs')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=DEVICE,\n",
    "        devices=NUM_DEVICES,\n",
    "        max_epochs=EPOCHS,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028630dd",
   "metadata": {},
   "source": [
    "Instantiating the model and the data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmodel = SegmentationModel(net, LOSS, LR)\n",
    "\n",
    "deepglobeData = DeepGlobeDataModule(train_df, valid_df, test_df, BATCH_SIZE, IMG_SIZE, preprocess_input, class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f19e74",
   "metadata": {},
   "source": [
    "Running our training based on a previously saved checkpoint if available, otherwise we start from scratch.\n",
    "This is also useful to load our model from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68707df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = '../input/savedckpt/'\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'lightning_trained-v1.ckpt') \n",
    "\n",
    "if os.path.isfile(checkpoint_file):\n",
    "    print('Resuming training from previous checkpoint...')\n",
    "    trainer.fit(segmodel, datamodule=deepglobeData,\n",
    "                ckpt_path=checkpoint_file)\n",
    "else:\n",
    "    print('Starting training from scratch...')\n",
    "    trainer.fit(segmodel, datamodule=deepglobeData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ed6d8",
   "metadata": {},
   "source": [
    "Loading our checkpoint and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315feddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT_DIR = '../input/saved-checkpoints/'\n",
    "# checkpoint_file = os.path.join(CHECKPOINT_DIR, 'lightning_trained-v1.ckpt')\n",
    "\n",
    "# segmodel = SegmentationModel.load_from_checkpoint(checkpoint_file)\n",
    "\n",
    "# deepglobeData = DeepGlobeDataModule(train_df, valid_df, test_df, \n",
    "#                                     BATCH_SIZE, IMG_SIZE, \n",
    "#                                     preprocess_input, class_rgb_values)\n",
    "\n",
    "trainer.test(segmodel, datamodule=deepglobeData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee136d57",
   "metadata": {},
   "source": [
    "Helper function to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48443ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c3916",
   "metadata": {},
   "source": [
    "Instantiating a test dataset for visualization (i.e without preprocessing function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testeset sem o preprocessamento para fins de visualização \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=IMG_SIZE),\n",
    "])\n",
    "\n",
    "testset = DeepGlobeSet(test_df, transform=transform, preprocess_fn=preprocess_input, class_rgb_values=class_rgb_values)\n",
    "testsetvis = DeepGlobeSet(test_df, transform=transform, preprocess_fn=None, class_rgb_values=class_rgb_values)\n",
    "\n",
    "reverse_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(size=IMG_SIZE),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820edeff",
   "metadata": {},
   "source": [
    "Visualizing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7279246",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    n = np.random.choice(len(testset))\n",
    "    \n",
    "    image, gt_mask = testset[n]\n",
    "    imagevis = testsetvis[n][0]\n",
    "    \n",
    "    # Creating a batch of size 1 (unsqueeze(0)) for image\n",
    "    pred = segmodel.forward(image.unsqueeze(0))\n",
    "    \n",
    "    # Getting pred tensor from gpu to cpu, squeezing it and casting it to numpy\n",
    "    pred = pred.cpu().detach().squeeze().numpy()\n",
    "    \n",
    "    # Getting gt_mask tensor from gpu to cpu and casting to uint8\n",
    "    gt_mask = gt_mask.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # reversing one_hot transformations (made in the Mining class object) of gt_mask and pred\n",
    "    reversed_gt_mask = reverse_one_hot(gt_mask)\n",
    "    reversed_pred = reverse_one_hot(pred)\n",
    "    \n",
    "    # reapplying the original colors in the reversed one hot images\n",
    "    # and getting them as uint8\n",
    "    ground_truth_mask = colour_code_segmentation(reversed_gt_mask, class_rgb_values).astype(np.uint8)\n",
    "    prediction = colour_code_segmentation(reversed_pred, class_rgb_values).astype(np.uint8)\n",
    "            \n",
    "    visualize(\n",
    "        image=reverse_transform(imagevis),\n",
    "        prediction=reverse_transform(prediction),\n",
    "        #one_hot_mask=reverse_transform(reversed_gt_mask.astype(np.uint8)),\n",
    "        ground_truth_mask=reverse_transform(ground_truth_mask.astype(np.uint8)),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 966962,
     "sourceId": 1635643,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2802407,
     "sourceId": 4835985,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 112.59635,
   "end_time": "2024-05-18T20:17:12.116931",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T20:15:19.520581",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
